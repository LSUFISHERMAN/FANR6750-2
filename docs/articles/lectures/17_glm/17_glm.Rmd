---
title: "LECTURE 17: generalized linear models"
subtitle: "FANR 6750 (Experimental design)"
author: "<br/><br/><br/>Fall 2023"
output:
  xaringan::moon_reader:
    css: ["default", "FANR6750.css", "FANR6750-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, echo = FALSE, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', warning=FALSE, message=FALSE, fig.retina = 2)
source(here::here("R/zzz.R"))
library(emo)
library(FANR6750)
library(dplyr)
library(kableExtra)
# library(gganimate)
```

# outline 


1) Motivation

<br/>

--

2) Linear model review

<br/>

--

3) Distributions

<br/>
--


4) Link functions

<br/>
--

5) Generalized linear models



---
# motivation

#### Although the central limit theorem often allows us to meet the normality assumption, there are many cases where the assumption does not hold:  

--

- Presence-absence studies  

- Studies of survival  

- Seed germination studies  

- Count-based surveys (especially when close to zero)

---
# linear model review

<br/>
<br/>
$$\Large response = deterministic\; part+stochastic\; part$$ 
<br/>
<br/>

--
$$\underbrace{\LARGE E[y_i] = \beta_0 + \beta_1 \times x_i}_{Deterministic}$$

<br/>
<br/>

--
$$\underbrace{\LARGE y_i \sim normal(E[y_i], \sigma)}_{Stochastic}$$  

---
# example

Imagine we are interested in the effects of seed size on germination rate

--

We measure 100 seeds and then plant them in a greenhouse. All seeds are kept at the same temperature, soil moisture, etc. After several weeks, we record which seeds germinated and which did not

```{r}
data("seeddata")
kable(seeddata[1:4,], format = 'html', col.names = c("Size", "Fate"), escape = FALSE, align = "c")
```

--

What is the response variable $y$ in this example? What are the possible values it can take?

--

What is the predictor variable $x$ in this example?

---
# example

<br/>

```{r fig.height=5, fig.width=7}
data("seeddata")
ggplot(seeddata, aes(x = size, y = fate)) +
  geom_point() +
  scale_y_continuous("Fate", breaks = c(0, 1), labels = c("Fail (0)", "Germinate (1)")) +
  scale_x_continuous("Size (mg)")
```

---
# example

.small-code[
```{r echo = TRUE}
data("seeddata")
fit <- lm(fate ~ size, data = seeddata)
summary(fit)
```
]

---
# example

<br/>

```{r fig.height=5, fig.width=7}
data("seeddata")
ggplot(seeddata, aes(x = size, y = fate)) +
  geom_point() +
  scale_y_continuous("Fate", breaks = c(0, 1)) +
  scale_x_continuous("Size (mg)") +
  stat_smooth(method = "lm")
```

--

What's wrong with the expected values? What is an "expected value" in this example?

---
# another example

We are interested in the effect of non-native Joro Spiders (*Trichonephila clavata*) on the number of native Golden Silk Orb Weaver Spiders (*Trichonephila clavipes*)

```{r}
data("spiderdata")
kable(spiderdata[1:4, ], format = 'html', col.names = c("T. clavata", "T. clavipes"), escape = FALSE, align = "c")
```

--

What is the response variable $y$ in this example? What are the possible values it can take?

--

What is the predictor variable $x$ in this example?

---
# another example

<br/>

```{r fig.height=5, fig.width=7}
ggplot(spiderdata, aes(x = clavata, y = clavipes)) +
  geom_point() +
  scale_y_continuous("T. clavipes (Orb Weavers)") +
  scale_x_continuous("T. clavata (Joro)")
```

---
# another example

.small-code[
```{r echo = TRUE}
data("spiderdata")
fit <- lm(clavipes ~ clavata, data = spiderdata)
summary(fit)
```
]

---
# another example

<br/>

```{r fig.height=5, fig.width=7}
ggplot(spiderdata, aes(x = clavata, y = clavipes)) +
  geom_point() +
  scale_y_continuous("T. clavipes (Orb Weavers)") +
  scale_x_continuous("T. clavata (Joro)")+
  stat_smooth(method = "lm")
```

---
class: inverse, middle

# generalized linear models

---
# generalized linear models

GLMs are a class of models that extend the conventional linear model framework to include data-generating distributions other than the normal distribution

--

All GLMs are composed of:

1) Distribution 

$$f(y_i|x_i)$$

--

2) Linear predictor

$$\mu_i = \beta_0 + \beta_1 x_i...$$

--

3) Link function

$$\eta_i = g(\mu_i)$$

--

The distribution allows us to model response variables that are not normally distributed. The link function allows us to map the linear predictor to the expected values of the probability distribution

---
# the linear model as a glm

#### Linear regression

--

- Distribution: $f(y_i|x_i) = normal(\mu_i, \sigma^2)$

--

- Linear predictor: $\mu_i = \beta_0 + \beta_1 x_i$

--

- Link function: $g(\mu_i) = \mu_i$ (identity link)

---
class: inverse, middle

# probability distributions

---
# probability distributions

There are *many* probability distributions. We've seen several this semester (what are they?)

--

The three that we will focus on for the remainder of the course are:

1) Poisson distribution

2) Binomial distribution

3) Bernoulli distribution

---
# poisson distribution

The Poisson distribution is a *discrete* probability distribution (only integers) with support (i.e., possible values) from 0 to $+\infty$. It is often used to model count data

--

The Poisson distribution has one parameter $\lambda$, which is both the mean and the variance

--

```{r fig.width=8, fig.height=4}
x <- seq(0, 50, by = 1)
df <- data.frame(y = c(dpois(x, lambda = 5), dpois(x, lambda = 20), dpois(x, lambda = 30)),
                 x = x,
                 lambda = factor(rep(c("lambda = 5", "lambda = 20", "lambda = 30"), each = length(x)),
                                 levels = c("lambda = 5", "lambda = 20", "lambda = 30")))
ggplot(df, aes(x = x, y = y, fill = lambda)) +
  geom_col(alpha = 0.75, color = "grey", linewidth = 0.1, position = "dodge") +
  scale_y_continuous("Probability density") +
  scale_x_continuous("y") +
  scale_fill_discrete(labels = c(expression(paste(lambda, "= 5")), 
                                 expression(paste(lambda, "= 20")), 
                                 expression(paste(lambda, "= 30"))))
```

--

Note that although $y$ must be an integer, $\lambda$ only has to be positive 

---
# poisson distribution

```{r echo = TRUE}
rpois(n = 10, lambda = 1.2)
```

```{r echo = TRUE}
rpois(n = 10, lambda = 10.5)
```

```{r echo = TRUE}
rpois(n = 10, lambda = 100.68)
```

---
# binomial distribution

The binomial distribution is also a discrete probability distribution with support from 0 to $N$. It is often used to model count data where there is an upper limit (e.g., # of heads from $N$ coin flips)

--

The binomial distribution has two parameters $N$ (# of trials) and $p$ (probability of success). The mean is $Np$ and the variance is $np(1-p)$

--

```{r fig.width=8, fig.height=4}
x <- seq(0, 40, by = 1)
df <- data.frame(y = c(dbinom(x, size = 5, prob = 0.25), dbinom(x, size = 10, prob = 0.8), dbinom(x, size = 20, prob = 0.5), dbinom(x, size = 35, prob = 0.8)),
                 x = x,
                 lambda = rep(c("N = 5, p = 0.25", "N = 10, p = 0.8", "N = 20, p = 0.5", "N = 35, p = 0.8"), each = length(x)))
ggplot(df, aes(x = x, y = y, fill = as.factor(lambda) )) +
  geom_col(alpha = 0.75, color = "grey", linewidth = 0.1, position = "dodge") +
  scale_y_continuous("Probability density") +
  scale_x_continuous("Number of successes") 

```

--

Note that $N$ is often fixed and known. We are interested in estimating $p$

---
# binomial distribution

```{r echo = TRUE}
rbinom(n = 10, size = 20, p = 0.25)
```

```{r echo = TRUE}
rbinom(n = 10, size = 20, p = 0.75)
```

```{r echo = TRUE}
rbinom(n = 10, size = 50, p = 0.5)
```

---
# bernoulli distribution

The Bernoulli distribution is a special case of the binomial distribution where $N = 1$. What are the possible values of $y$? 

--

The Bernoulli distribution is very useful for data where the response variable is binary in nature (e.g., dead/alive, presence/absence, ). The parameter we estimate is $p$ - the probability of "success"

```{r fig.width=8, fig.height=4}
x <- c(0,1)
df <- data.frame(y = c(dbinom(x, size = 1, prob = 0.2), dbinom(x, size = 1, prob = 0.5), dbinom(x, size = 1, prob = 0.8)),
                 x = x,
                 p = rep(c("p = 0.2", "p = 0.5", "p = 0.8"), each = length(x)))
ggplot(df, aes(x = x, y = y, fill = as.factor(p) )) +
  geom_col(alpha = 0.75, width = 0.1,color = "grey", linewidth = 0.1, position = "dodge") +
  scale_y_continuous("Probability density") +
  scale_x_continuous("", breaks = c(0, 1)) 

```

---
# bernoulli distribution

```{r echo = TRUE}
rbinom(n = 10, size = 1, p = 0.1)
```

```{r echo = TRUE}
rbinom(n = 10, size = 1, p = 0.5)
```

```{r echo = TRUE}
rbinom(n = 10, size = 1, p = 0.9)
```

---
class: inverse, middle

# link functions

---
# link functions

In a conventional linear regression model, the expected value of $y_i$ given $x_i$ is simply:

$$\large E[y_i|x_i] = \mu_i = \beta_0 + \beta_1x_i$$
<br/>

```{r fig.height=4, fig.width=6}
x <- runif(100, 0, 10)
beta <- c(-2, 1)
ey <- cbind(1, x) %*% beta
y <- rnorm(100, ey, 1)
df <- data.frame(x = x, y = y)
ggplot(df, aes(x = x, y = y)) +
  geom_point()+
  stat_smooth(method = "lm")
```

---
# link functions

When the data follow distributions other than the normal, we need to use a transformation to ensure that the linear predictor $\mu_i$ is consistent with the parameters governing the mean of the distribution

--

- For the Poisson distribution, the expected value $\lambda_i$ must be positive

--

- For the binomial/Bernoulli distributions, the probability of success $p_i$ must be between 0 and 1

--

Link functions allow us to go back and forth between the linear scale ( $-\infty$ to $+\infty$ ) and the scale of the Poisson/binomial distributions


---
# link functions

<br/>

```{r}
link <- data.frame(Distribution = c("Binomial", "Poisson"),
                   link = c("logit", "log"),
                   equation = c('\\(\\mu = log(\\frac{p}{1-p})\\)',
                                '\\(\\mu = log(\\lambda)\\)'),
                   inverse = c('\\(p = \\frac{e^{\\mu}}{1+e^{\\mu}}\\)',
                                '\\(\\lambda = e^{\\mu}\\)'))

link %>%
  kable(format = 'html', col.names = c("Distribution", "link name", "link equation", "inverse link equation"), escape = FALSE, align = "c")
```

<br/>

--

```{r}
linkR <- data.frame(Distribution = c("Binomial", "Poisson"),
                   link = c("logit", "log"),
                   equation = c("qlogis", "log"),
                   inverse = c("plogis", "exp"))

linkR %>%
  kable(format = 'html', col.names = c("Distribution", "link name", "link in R", "inverse link in R"), align = "c")
```

---
# log link

.small-code[
```{r echo = TRUE}
beta0 <- 5
beta1 <- -0.08
(log.lambda <- beta0 + beta1*100) # linear predictor for x = 100
```
]

--

How do we convert -3 to a positive value? Use the inverse-link:

.small-code[
```{r echo = TRUE}
(lambda <- exp(log.lambda))
```
]

--

To go back, use the link function itself:

.small-code[
```{r echo =TRUE}
log(lambda)
```
]

---
# log link

<br/>

```{r fig.height=6, fig.width=6}
df <- data.frame(mu = seq(-3, 3, length.out = 100),
                 lambda = exp(seq(-3, 3, length.out = 100)))
ggplot(df, aes(x = mu, y = lambda)) +
  geom_path() +
  scale_x_continuous(expression(paste(mu, "=log(", lambda, ")"))) +
  scale_y_continuous(expression(paste(lambda, "=exp(", mu, ")")))
```

---
# log link example

.pull-left[
<br/>
```{r fig.height=4, fig.width=4}
ggplot(spiderdata, aes(x = clavata, y = clavipes)) +
  geom_point() +
  scale_y_continuous("T. clavipes (Orb Weavers)") +
  scale_x_continuous("T. clavata (Joro)")+
  stat_smooth(method = "lm")+
  labs(title = expression(beta[0]+beta[1] * x[i]))
```
]

--

.pull-right[
<br/>
```{r fig.height=4, fig.width=4}
ggplot(spiderdata, aes(x = clavata, y = clavipes)) +
  geom_point() +
  scale_y_continuous("") +
  scale_x_continuous("T. clavata (Joro)")+
  stat_smooth(method = "glm", method.args = list(family = "poisson"))+
  labs(title = expression(paste("exp(", beta[0]+beta[1] * x[i], ")")))
```
]

---
# logit link

.small-code[
```{r echo = TRUE}
beta0 <- 5
beta1 <- -0.08
(logit.p <- beta0 + beta1*100) # linear predictor for x = 100
```
]

--

How do we convert -3 to a probability? Use the inverse-link:

.small-code[
```{r echo = TRUE}
(p <- exp(logit.p)/(1+exp(logit.p))) # same as plogis(logit.p)
```
]

--

To go back, use the link function itself:

.small-code[
```{r echo =TRUE}
log(p/(1-p))
qlogis(p)
```
]

---
# log link

<br/>

```{r fig.height=6, fig.width=6}
df <- data.frame(mu = seq(-5, 5, length.out = 100),
                 p = plogis(seq(-5, 5, length.out = 100)))
ggplot(df, aes(x = mu, y = p)) +
  geom_path() +
  scale_x_continuous(expression(paste(mu, "=logit(p)"))) +
  scale_y_continuous(expression(paste("p=ilogit(", mu, ")")))
```

---
# logit link example

.pull-left[
<br/>
```{r fig.height=4, fig.width=4}
ggplot(seeddata, aes(x = size, y = fate)) +
  geom_point() +
  scale_y_continuous("Probability of germintation", breaks = c(0, 1)) +
  scale_x_continuous("Size (mg)") +
  stat_smooth(method = "lm") +
  labs(title = expression(beta[0]+beta[1] * x[i]))
```
]

--

.pull-right[
<br/>
```{r fig.height=4, fig.width=4}
ggplot(seeddata, aes(x = size, y = fate)) +
  geom_point() +
  scale_y_continuous("", breaks = c(0, 1)) +
  scale_x_continuous("Size (mg)") +
  stat_smooth(method = "glm", method.args = list(family = "binomial"))+
  labs(title = expression(paste("ilogit(", beta[0]+beta[1] * x[i], ")")))
```
]


---
# generalized linear models

Putting the distribution and link functions together, we can write several of the most common GLMs

--

#### Logistic regression

--

- Distribution: $f(y_i|x_i) = Bernoulli(p_i)$

--

- Linear predictor: $\mu_i = \beta_0 + \beta_1 x_i$

--

- Link function: $g(p_i) = logit(p_i) = \mu_i$ (or $p_i = ilogit(\mu_i)$)

---
# generalized linear models

Putting the distribution and link functions together, we can write several of the most common GLMs


#### Poisson regression

--

- Distribution: $f(y_i|x_i) = Poisson(\lambda_i)$

--

- Linear predictor: $\mu_i = \beta_0 + \beta_1 x_i$

--

- Link function: $g(\lambda_i) = log(\lambda_i) = \mu_i$ (or $\lambda_i = exp(\mu_i)$)

---
# looking ahead

<br/>

### **Next time**: Logistic regression

<br/>

### **Reading**: [Fieberg chp. 16](https://statistics4ecologists-v1.netlify.app/logistic)


